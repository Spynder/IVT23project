Мы в основном работали с векторами, что записывались как $m$ чисел в виде столбика:
$$\left(\array{a_1 \\ a_2 \\ \ldots \\ a_m}\right)$$
Матрица - это несколько таких столбиков вместе!
$$\left(\array{
a_{11} & a_{12} & \ldots & a_{1n} \\ 
a_{21} & a_{22} & \ldots & a_{2n} \\ 
\vdots & \vdots & \ddots & \vdots \\ 
a_{m1} & a_{m2} & \ldots & a_{mn}
}\right)$$
Такая матрица будет размером $m \cross n$. Первым обозначается количество строк (или высота столбика, ведь матрица в первую очередь - группа столбиков), и уже потом количество столбцов.

## Определения видов матриц
1. Для матриц определена операция *транспонирования* - это обмен столбиков и рядов между собой - как будто мы по диагонали ее переворачиваем. Для матрицы $A$, транспонированная матрица обозначается как $A^T$. Матрица размеров $m \cross n$ при транспонировании станет матрицей $n \cross m$. Из определения вытекает очевидное равенство: $A^T_{ji} = A_{ij}$.
$$\left(\array{
a_{11} & a_{12} & a_{13} & \ldots & a_{1n} \\ 
a_{21} & a_{22} & a_{23} & \ldots & a_{2n} \\ 
\vdots & \vdots & \vdots & \ddots & \vdots \\ 
a_{m1} & a_{m2} & a_{m3} & \ldots & a_{mn}
}\right) \rightarrow \left(\array{
a_{11} & a_{21} & \ldots & a_{m1} \\ 
a_{12} & a_{22} & \ldots & a_{m2} \\ 
a_{13} & a_{23} & \ldots & a_{m3} \\ 
\vdots & \vdots & \ddots & \vdots \\ 
a_{1n} & a_{2n} & \ldots & a_{mn}
}\right)$$
2. Матрицы делятся на квадратные и прямоугольные. Квадратные, очевидно, когда значения $n$ и $m$ равны ($1 \cross 1,\ \  2 \cross 2, \ \ 3 \cross 3,\ \  \ldots$). Во всех остальных случаях считается, что матрица прямоугольная.
3. Для квадратных матриц определены понятия "главная диагональ" и "побочная диагональ".
	- Главная диагональ идет из левого верхнего угла в правый нижний. То есть элементы $a_{ii}$ для $0 < i \leq n$. $$\left(\array{
[a_{11}] & a_{12} & \ldots & a_{1(n-1)} & a_{1n} \\ 
a_{21} & [a_{22}] & \ldots & a_{2(n-1)}& a_{2n} \\ 
\vdots & \vdots & \ddots & \vdots & \vdots \\ 
a_{(n-1)1} & a_{(n-1)2} & \ldots & [a_{(n-1)(n-1)}] & a_{(n-1)n} \\
a_{n1} & a_{n2} & \ldots & a_{n(n-1)} & [a_{nn}]
}\right)$$
	- Побочная диагональ же наоборот - из правого верхнего угла в левый нижний. Это будут элементы $a_{i(n+1-i)}$ для $0 < i \leq n$.$$\left(\array{
a_{11} & a_{12} & \ldots & a_{1(n-1)} & [a_{1n}] \\ 
a_{21} & a_{22} & \ldots & [a_{2(n-1)}]& a_{2n} \\ 
\vdots & \vdots & \ddots & \vdots & \vdots \\ 
a_{(n-1)1} & [a_{(n-1)2}] & \ldots & a_{(n-1)(n-1)} & a_{(n-1)n} \\
[a_{n1}] & a_{n2} & \ldots & a_{n(n-1)} & a_{nn}
}\right)$$

4. Квадратные матрицы делятся на диагональные и недиагональные (остальные). У диагональных матриц все элементы, не лежащих на главной диагонали, равны нулю:$$\left(\array{a_{1} &0 & \ldots & 0 \\0 & a_{2} & \ldots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \ldots & a_{n}}\right)$$
5. Различают треугольные квадратные матрицы; а именно, верхние треугольные и нижние треугольные. Верхняя треугольная получается, когда числа образуют треугольник "сверху" - выше верхней диагонали (включительно), а все ниже заполнено нулями:$$\left(\array{
a_{11} & a_{12} & a_{13} &  \ldots & a_{1n} \\
0 & a_{22} & a_{23} & \ldots & a_{2n} \\
0 & 0 & a_{33} & \ldots & a_{2n} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \ldots & a_{nn}
}\right)$$Соответственно, нижняя треугольная образует треугольник "снизу":$$\left(\array{
a_{11} & 0 & 0 &  \ldots & 0 \\
a_{21} & a_{22} & 0 & \ldots & 0 \\
a_{31} & a_{32} & a_{33} & \ldots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
a_{n1} & a_{n2} & a_{n3} & \ldots & a_{nn}
}\right)$$
6. Квадратная матрица называается симметричной, если для любых $i,j$ выполняется равенство $A_{ij} = A_{ji}$. "Симметрия" матрицы здесь подразумевается относительно главной диагонали. $$\left(\array{
a_{11} & a_{12} & \ldots & a_{1n} \\ 
a_{12} & a_{22} & \ldots & a_{2n} \\ 
\vdots & \vdots & \ddots & \vdots \\ 
a_{1n} & a_{2n} & \ldots & a_{nn}
}\right)$$
## Линейные свойства матриц
Напомню, что такое линейность - сложение и умножение на скаляр (число). Соответственно, умножение двух матриц не является линейной операцией.

*Сложение* двух матриц происходит поэлементно, аналогично с векторами. Раз операция происходит поэлементно, то для каждого элемента в другой матрице должна найтись пара, поэтому возникает условие: для сложения, матрицы должны быть одинакого размера.
$$A + B = C \Leftrightarrow A_{ij} + B_{ij} = C_{ij}$$
$$\left(\array{a & b \\ c & d}\right) + \left(\array{e & f \\ g & h}\right) = \left(\array{a+e & b+f \\ c+g & d+h}\right)$$

Умножение на скаляр тоже происходит поэлементно, все так же аналогично с векторами (матрицы это же кучка векторов вместе):
$$\alpha A = B \Leftrightarrow \alpha A_{ij} = B_{ij}$$
$$\alpha \left(\array{a & b \\ c & d}\right) = \left(\array{\alpha a & \alpha b \\ \alpha c & \alpha d}\right)$$
Из определений этих двух линейных операций, вытекают свойства, напрямую выходящие из свойств сложения и умножения обычных чисел. Заглавными буквами обозначаются матрицы (одинаковых размеров), греческими буквами обозначаются скаляры:
- Коммутативность: $A + B = B + A$
- Ассоциативность: $(A+B)+C = A+(B+C)$
- Нейтральный элемент: $A + 0 = A$ (здесь $0$ - нулевая матрица)
- Обратный элемент: $A + (-A) = 0$ ($-A = (-1)A$, называется *противоположной* матрицей - не путать с *обратной*)

- Ассоциативность: $(\alpha \beta)A = \alpha (\beta A) = \beta (\alpha A)$
- Дистрибутивность: $(\alpha + \beta)A = \alpha A + \beta A$
- Еще одна дистрибутивность: $\alpha (A + B) = \alpha A + \alpha B$
- Нейтральный элемент: $1 \cdot A = A$.
## Линейная зависимость матриц
Помните, как было понятие линейной зависимости системы векторов? [[#3. Векторные пространства. Линейно зависимые и линейно независимые системы векторов. Их свойства.]]
Мы говорили, что система векторов зависима, когда один из векторов представляется как линейная комбинация других векторов.
Точно такое же и с матрицами: система матриц называется *линейно зависимой*, если одна из матриц может быть представлена как линейная комбинация других матриц системы. В противном случае, система матриц будет *линейно независимой*.

Строго математически, этот критерий можно переформулировать: система называется *линейно зависимой*, когда существует нетривиальная линейная комбинация, что:
$$a_1A_1 + a_2A_2 + \ldots + a_kA_k = 0$$
Если единственное решение этой системы тривиально (все коэффициенты $a_i = 0$), то система называется *линейно независимой*.

Свойства линейно зависимых и независимых систем матриц совпадает с соответующими системами векторов (ведь матрицы, опять же, просто группы векторов).
## Пример задачи
Условие: определите, линейно зависимая ли система матриц $$A = \left(\array{2&-1\\3&4}\right) \ \ \ \ \ B = \left(\array{-4&2\\-6&-8}\right)$$

Решение: если система линейно зависимая, существует такое $\alpha$, что $\alpha A = B$.
$$\left(\array{2\alpha & -\alpha \\ 3\alpha & 4\alpha}\right) = \left(\array{-4&2\\-6&-8}\right)$$
Матрицы равны, когда равны все их элементы. Соответственно:
$$\cases{2\alpha = -4 \\ -\alpha = 2 \\ 3\alpha = -6 \\ 4a = -8} \Rightarrow \alpha = -2$$

Ответ: система линейно зависима. А именно, $-2 A = B$.

---
Условие: определите, линейно зависимая ли система матриц $$A = \left(\array{2&-1\\3&4}\right) \ \ \ \ \ B = \left(\array{5&1\\8&4}\right) \ \ \ \ \ C = \left(\array{-5&4\\7&2}\right)$$

Решение: чтобы система была линейно зависимой, должны существовать $\alpha, \beta$, что $\alpha A + \beta B = C$.
По аналогии с прошлым решением:
$$\left(\array{2\alpha & -\alpha \\ 3\alpha & 4\alpha}\right) + \left(\array{5\beta & -\beta \\ 8\beta & 4\beta}\right)= \left(\array{-5 & 4\\7&2}\right)$$
В систему:
$$\cases{2\alpha + 5\beta = -5 \\ -\alpha - \beta = 4 \\ 3\alpha + 8\beta = 7 \\ 4\alpha + 4\beta = 2} \Rightarrow \varnothing$$
Можно легко увидеть, что решения нет, если взглянуть на строки 2 и 4. Раз решений нет, то...

Ответ: система линейно независима.
